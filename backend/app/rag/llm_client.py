"""
LiteLLM client for Gemini embeddings and completions.
Reference: https://github.com/groq-ai/litellm
"""
import logging
import os
from typing import List

import litellm
from dotenv import load_dotenv
from pydantic import BaseModel

load_dotenv()
# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class LitellmClient:
    """Client for LiteLLM operations with Gemini models."""
    
    def __init__(self, api_key: str = None):
        """Initialize LiteLLM client with API key."""
        self.api_key = api_key or os.getenv("GEMINI_API_KEY")
        
        logger.info("üîë Initializing LiteLLM Client...")
        
        if not self.api_key:
            logger.error("‚ùå GEMINI_API_KEY environment variable is required!")
            raise ValueError("GEMINI_API_KEY environment variable is required")
        
        # Check if API key is valid format (not empty, has reasonable length)
        if len(self.api_key) < 10:
            logger.error("‚ùå API key seems too short. Please check your GEMINI_API_KEY")
            raise ValueError("Invalid API key format")
        
        logger.info(f"‚úÖ API Key found: {self.api_key[:10]}...{self.api_key[-4:]}")
        
        # Configure LiteLLM
        litellm.api_key = self.api_key
        logger.info("üîß LiteLLM configured with API key")
    
    async def embed_texts(self, texts: List[str]) -> List[List[float]]:
        """
        Generate embeddings for a list of texts using Gemini embeddings.
        
        Args:
            texts: List of text strings to embed
            
        Returns:
            List of embedding vectors
        """
        logger.info(f"üß† Generating embeddings for {len(texts)} text(s)")
        
        for i, text in enumerate(texts):
            logger.info(f"üìù Text {i+1}: {text[:100]}{'...' if len(text) > 100 else ''}")
        
        try:
            logger.info("üöÄ Calling LiteLLM embedding API...")
            embeddings = await litellm.embedding(
                model="gemini/gemini-embedding-001",
                input=texts
            )
            
            # Extract embedding vectors
            embedding_vectors = [embedding["embedding"] for embedding in embeddings["data"]]
            
            logger.info(f"‚úÖ Successfully generated {len(embedding_vectors)} embeddings")
            logger.info(f"üìä Embedding dimensions: {len(embedding_vectors[0]) if embedding_vectors else 0}")
            
            # Log some stats about the embeddings
            for i, embedding in enumerate(embedding_vectors):
                logger.info(f"üìà Embedding {i+1} stats - Length: {len(embedding)}, "
                          f"Min: {min(embedding):.4f}, Max: {max(embedding):.4f}, "
                          f"Mean: {sum(embedding)/len(embedding):.4f}")
            
            return embedding_vectors
            
        except Exception as e:
            logger.error(f"‚ùå Embedding generation failed: {str(e)}")
            logger.error(f"üîç Error type: {type(e).__name__}")
            raise Exception(f"Embedding generation failed: {str(e)}")
    
    async def generate_answer(self, prompt: str) -> str:
        """
        Generate a completion using Gemini Ultra.
        
        Args:
            prompt: The input prompt
            
        Returns:
            Generated text response
        """
        logger.info("ü§ñ Generating completion with Gemini Ultra...")
        logger.info(f"üìù Prompt length: {len(prompt)} characters")
        logger.info(f"üìÑ Prompt preview: {prompt[:200]}{'...' if len(prompt) > 200 else ''}")
        
        try:
            logger.info("üöÄ Calling LiteLLM completion API...")
            response = await litellm.completion(
                model="gemini/gemini-2.5-flash",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.1,
                max_tokens=1000
            )
            
            answer = response.choices[0].message.content
            logger.info(f"‚úÖ Successfully generated completion")
            logger.info(f"üìù Response length: {len(answer)} characters")
            logger.info(f"üí¨ Response preview: {answer[:200]}{'...' if len(answer) > 200 else ''}")
            
            # Log usage statistics if available
            if hasattr(response, 'usage'):
                usage = response.usage
                logger.info(f"üìä Usage - Tokens: {usage.total_tokens}, "
                          f"Prompt: {usage.prompt_tokens}, "
                          f"Completion: {usage.completion_tokens}")
            
            return answer
            
        except Exception as e:
            logger.error(f"‚ùå Completion generation failed: {str(e)}")
            logger.error(f"üîç Error type: {type(e).__name__}")
            raise Exception(f"Completion generation failed: {str(e)}")
    
    def test_connection(self) -> bool:
        """Test if the LLM client can connect and authenticate."""
        logger.info("üß™ Testing LLM client connection...")
        
        try:
            # Test with a simple embedding
            import asyncio
            
            async def test():
                test_texts = ["Hello, world!"]
                embeddings = await self.embed_texts(test_texts)
                return len(embeddings) > 0 and len(embeddings[0]) > 0
            
            result = asyncio.run(test())
            
            if result:
                logger.info("‚úÖ LLM client connection test passed!")
                return True
            else:
                logger.error("‚ùå LLM client connection test failed!")
                return False
                
        except Exception as e:
            logger.error(f"‚ùå LLM client connection test failed: {str(e)}")
            return False


# Global client instance
llm_client = LitellmClient()
